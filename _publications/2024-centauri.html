---
layout: publication
year: 2024
title: "Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning"
authors:
  - Chang Chen
  - Xiuhong Li
  - Qianchao Zhu
  - Jiangfei Duan
  - Peng Sun
  - Xingcheng Zhang
  - Chao Yang
type:
  - Conference
venue: ASPLOS
venue_location: San Diego, USA
venue_tags:
  - ASPLOS
tags:
  - Large Language Models Training
  - Scheduling
  - Hybrid Parallel Methods
  - Communication-Computation Overlap 
venue_url: https://www.asplos-conference.org/asplos2024/
doi: "10.1145/3620666.3651379"
pdf: /assets/publications/2024-centauri.pdf
html: https://dl.acm.org/doi/10.1145/3620666.3651379
---

Efficiently training large language models (LLMs) necessitates the adoption of hybrid parallel methods, integrating multiple communications collectives within distributed partitioned graphs. Overcoming communication bottlenecks is crucial and is often achieved through communication and computation overlaps. However, existing overlap methodologies tend to lean towards either fine-grained kernel fusion or limited operation scheduling, constraining performance optimization in heterogeneous training environments.

This paper introduces Centauri, an innovative framework that encompasses comprehensive communication partitioning and hierarchical scheduling schemes for optimized overlap. We propose a partition space comprising three inherent abstraction dimensions: primitive substitution, topology-aware group partitioning, and workload partitioning. These dimensions collectively create a comprehensive optimization space for efficient overlap. To determine the efficient overlap of communication and computation operators, we decompose the scheduling tasks in hybrid parallel training into three hierarchical tiers: operation, layer, and model. Through these techniques, Centauri effectively overlaps communication latency and enhances hardware utilization. Evaluation results demonstrate that Centauri achieves up to 1.49Ã— speedup over prevalent methods across various parallel training configurations.

